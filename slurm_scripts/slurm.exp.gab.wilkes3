#!/bin/bash
#SBATCH -J gab-exp
#SBATCH -A MLMI-SW984-SL2-GPU
#! How many whole nodes should be allocated?
#SBATCH --nodes=1
#! How many (MPI) tasks will there be in total?
#! Note probably this should not exceed the total number of GPUs in use.
#SBATCH --ntasks=1
#! Specify the number of GPUs per node (between 1 and 4; must be 4 if nodes>1).
#! Note that the job submission script will enforce no more than 32 cpus per GPU.
#SBATCH --gres=gpu:1
#SBATCH --time=02:30:00
#SBATCH -p ampere
#SBATCH --output=slurm_logs/%j.%x_%a.out
#SBATCH --error=slurm_logs/%j.%x_%a.err
#SBATCH --array=0-7

#! Optionally modify the environment seen by the application
#! (note that SLURM reproduces the environment at submission irrespective of ~/.bashrc):
. /etc/profile.d/modules.sh                # Leave this line (enables the module command)
module purge                               # Removes all modules still loaded
module load rhel8/default-amp              # REQUIRED - loads the basic environment

# Additional environment setup
module load python/3.8
source .venv/bin/activate

# Start logs
echo -e "JobID: $SLURM_JOB_ID\n======"
echo "Running on master node: `hostname`"
echo "Start time: `date`"

#################################

EXP_DIR="experiments/Gab"
INPUTS=data/splits/gab/test.inputs.txt
REFS=data/splits/gab/test.references.txt

NO_REP=5
BATCH_SIZE=8
MIN_LEN=20

#################################

SYSTEM_NAMES=(DGPT-Gab DGPT-Gab-MC DGPT-Gab-MC-minlen$MIN_LEN DGPT-MC DGPT-MC-minlen$MIN_LEN DGPT DGPT-minlen$MIN_LEN Gold-standard)
MODEL_NAMES=(models/DialoGPT-finetuned-gab models/DialoGPT-finetuned-gab-multiCONAN models/DialoGPT-finetuned-gab-multiCONAN models/DialoGPT-finetuned-multiCONAN models/DialoGPT-finetuned-multiCONAN microsoft/DialoGPT-medium microsoft/DialoGPT-medium NA)
DECODE_OR_NOT_ARRAY=(false false false false false false false NA)
MIN_NEW_TOKENS_ARRAY=(None None $MIN_LEN None $MIN_LEN None $MIN_LEN None)

SYSTEM_NAME=${SYSTEM_NAMES[$SLURM_ARRAY_TASK_ID]}
MODEL_NAME=${MODEL_NAMES[$SLURM_ARRAY_TASK_ID]}
SHOULD_DECODE=${DECODE_OR_NOT_ARRAY[$SLURM_ARRAY_TASK_ID]}
MIN_NEW_TOKENS=${MIN_NEW_TOKENS_ARRAY[$SLURM_ARRAY_TASK_ID]}

OUTPUT_DIR="$EXP_DIR/$SYSTEM_NAME"

if [ $SHOULD_DECODE = true ]; then
    CONFIG_OVERRIDES="no_repeat_ngram_size=$NO_REP;min_new_tokens=$MIN_NEW_TOKENS"
    python decode.py -m $MODEL_NAME -o $OUTPUT_DIR -co $CONFIG_OVERRIDES -i $INPUTS -b $BATCH_SIZE
fi

PREDICTIONS_FILE="$OUTPUT_DIR/predictions.txt"
python evaluate.py -p $PREDICTIONS_FILE -v -r $REFS -i $INPUTS

################################

bash extract-experiment-results.sh $EXP_DIR

# Final logs
echo "Finish time: `date`"
